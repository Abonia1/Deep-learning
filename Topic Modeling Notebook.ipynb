{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e5753c",
   "metadata": {},
   "source": [
    "# Topic Modeling for Intent Recommendation\n",
    "\n",
    "## 1. Configuration and setup\n",
    "\n",
    "### 1.1 Installation\n",
    "As Watson Studio is creating a new environment for each run, we have to download BERTopic and Spacy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c01ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c5b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd6abf8",
   "metadata": {},
   "source": [
    "### 1.2 Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tempfile import TemporaryFile\n",
    "import io\n",
    "from io import StringIO\n",
    "import joblib\n",
    "import numpy as np\n",
    "import ibm_boto3\n",
    "from botocore.client import Config\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import spacy\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5de9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If an error appear, just RERUN THE CELL\n",
    "#TO DO: see how to avoir the error, to be able to run Jobs\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-evening",
   "metadata": {},
   "source": [
    "### 1.3 Credentials & Watson Assistant configuration\n",
    "\n",
    "This notebook uses Watson Assistant v1 API to access skill definition. To access message logs, the notebook uses both v1 and v2 APIs. You authenticate to the API by using IBM Cloud Identity and Access Management (IAM).\n",
    "\n",
    "You can access the values you need for this configuration from the Watson Assistant user interface. Go to the Skills page and select View API Details from the menu of a skill title.\n",
    "\n",
    "- The string to set in the call to `IAMAuthenticator` is your Api Key under Service Credentials\n",
    "- The string to set for version is a date in the format version=YYYY-MM-DD. The version date string determines which version of the Watson Assistant V1 API will be called. For more information about version, see [Versioning](https://cloud.ibm.com/apidocs/assistant/assistant-v1#versioning).\n",
    "- The string to pass into `assistant.set_service_url` is the base URL of Watson Assistant. For example, for us-south, the endpoint is `https://api.us-south.assistant.watson.cloud.ibm.com`. This value will be different depending on the location of your service instance. For more information, see [Service Endpoint](https://cloud.ibm.com/apidocs/assistant/assistant-v1?code=python#service-endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-empty",
   "metadata": {},
   "source": [
    "### 1.4 Cloud Object Storage functions\n",
    "Cloud Object Storage provide the ressource to fetch/save an object. They are used with these functions : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resource\n",
    "#For \"storage\" bucket\n",
    "cos = ibm_boto3.resource(\"s3\",\n",
    "    ibm_api_key_id=COS_API_KEY_ID,\n",
    "    ibm_service_instance_id=COS_RESOURCE_CRN,\n",
    "    ibm_auth_endpoint=COS_AUTH_ENDPOINT,\n",
    "    config=Config(signature_version=\"oauth\"),\n",
    "    endpoint_url=COS_ENDPOINT\n",
    ")\n",
    "\n",
    "#This is for \"do_not_delete\" Bucket \n",
    "cos2 = ibm_boto3.resource(\"s3\",\n",
    "    ibm_api_key_id=COS_API_KEY_ID,\n",
    "    ibm_service_instance_id=COS_RESOURCE_CRN,\n",
    "    ibm_auth_endpoint=COS_AUTH_ENDPOINT,\n",
    "    config=Config(signature_version=\"oauth\"),\n",
    "    endpoint_url=COS_ENDPOINT2 #you have to change it depending on your bucket location \n",
    ")\n",
    "\n",
    "#Function to get file from \"do_not_delete\" Bucket\n",
    "def get_item_cos_donotdelete(bucket_name, item_name):\n",
    "    print(\"Retrieving item from bucket: {0}, key: {1}\".format(bucket_name, item_name))\n",
    "    try:\n",
    "        file = cos2.Object(bucket_name, item_name).get()\n",
    "        #print(\"File Contents: {0}\".format(file[\"Body\"].read()))\n",
    "        #print(pd.read_json(file[\"Body\"]))\n",
    "        return(file[\"Body\"].read())\n",
    "    except ClientError as be:\n",
    "        print(\"CLIENT ERROR: {0}\\n\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to retrieve file contents: {0}\".format(e))\n",
    "        \n",
    "#Function to get file from \"storage\" Bucket\n",
    "def get_item_cos(bucket_name, item_name):\n",
    "    print(\"Retrieving item from bucket: {0}, key: {1}\".format(bucket_name, item_name))\n",
    "    try:\n",
    "        file = cos.Object(bucket_name, item_name).get()\n",
    "        #print(\"File Contents: {0}\".format(file[\"Body\"].read()))\n",
    "        #print(pd.read_json(file[\"Body\"]))\n",
    "        return(file[\"Body\"].read())\n",
    "    except ClientError as be:\n",
    "        print(\"CLIENT ERROR: {0}\\n\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to retrieve file contents: {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05221cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get Bucket content, filter by prefix \n",
    "\n",
    "def get_bucket_contents_prefix(bucket_name, prefix):\n",
    "    print(\"Retrieving bucket contents from: {0}\".format(bucket_name))\n",
    "    try:\n",
    "        files = cos.Bucket(bucket_name).objects.filter(Prefix=prefix)\n",
    "        for file in files:\n",
    "            print(\"Item: {0} ({1} bytes).\".format(file.key, file.size))\n",
    "    except ClientError as be:\n",
    "        print(\"CLIENT ERROR: {0}\\n\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to retrieve bucket contents: {0}\".format(e))\n",
    "    #need to add a return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-patch",
   "metadata": {},
   "source": [
    "## 2. Load data from Cloud Object Storage \n",
    "### 2.1 Fetch and load data from a file (to delete )\n",
    "For the moment the input is \"The grand d√©bat\", from a French politic public form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = get_item_cos(BUCKET,'grand_debat_bert.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f04680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Loading data from CSV\n",
    "\n",
    "import os, types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "df = pd.read_csv(body,  sep= ',', index_col=[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7066cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_excel(file)\n",
    "data = df[\"text\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are just keeping a sample\n",
    "\n",
    "data = data[:10000] #used for Bertopic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0f6fa",
   "metadata": {},
   "source": [
    "### 2.2 Fetch and load data from  Analysis output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = df['request_input'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a4a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-newfoundland",
   "metadata": {},
   "source": [
    "# Topic Modeling with Bertopic and sentence transformer \n",
    "### Embedding \n",
    "Regarding to the pretrained model performance https://www.sbert.net/docs/pretrained_models.html, we are choosing the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dafd8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sentence_transformers_models=['distiluse-base-multilingual-cased-v1','paraphrase-multilingual-mpnet-base-v2 ','paraphrase-multilingual-MiniLM-L12-v2','paraphrase-MiniLM-L3-v2','multi-qa-mpnet-base-dot-v1','multi-qa-distilbert-cos-v1','multi-qa-MiniLM-L6-cos-v1','distiluse-base-multilingual-cased-v2']\n",
    "sentence_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "#sentence_model2 = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to avoid computation troubles during Bertopic() execution\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-subsection",
   "metadata": {},
   "source": [
    "### BERTopic Parameters: \n",
    "https://colab.research.google.com/drive/1ClTYut039t-LDtlcd-oQAdXWgcsSGTw9?usp=sharing#scrollTo=xG_slPMurnmz\n",
    "\n",
    "- n_gram_range : The n-gram range for the CountVectorizer. \n",
    " Advised to keep high values between 1 and 3. More would likely lead to memory issues. \n",
    " NOTE: This param will not be used if you pass in your own CountVectorizer.\n",
    "default value = (1, 1)\n",
    "\n",
    "\n",
    "- top_n_word : keep between 10 and 20, no more than 30\n",
    "top_n_words refers to the number of words per topic that you want extracted.\n",
    "\n",
    "\n",
    "- min_topic_size : is an important parameter! \n",
    "It is advised to play around with this value depending on the size of the your dataset. \n",
    "default value = 10\n",
    "\n",
    "#### For topic reduction:\n",
    "\n",
    "- nr_topics : Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. \n",
    "If this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics using HDBSCAN.\n",
    "None (default value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# to remove stopword\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "stopwords = en_stop.union(fr_stop)\n",
    "#We are adding a vectorizer to deal with stopword\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=list(stopwords))\n",
    "\n",
    "#Then we are creating a model for each embedding model:\n",
    "\n",
    "#all-mpnet-base-v2\n",
    "topic_model = BERTopic(embedding_model=sentence_model,\n",
    "                      verbose=True,\n",
    "                      n_gram_range=(1,3), \n",
    "                      nr_topics=\"auto\", #auto is using HDBSCAN \n",
    "                      vectorizer_model=vectorizer_model, #better result with our vectorizer\n",
    "                      top_n_words = 15,\n",
    "                      min_topic_size = 100, #should depend from the dataset size \n",
    "                      low_memory=False, #True = low memory for computation, so longer\n",
    "                      calculate_probabilities=True #calculate prob of each topic, is computationally expensive\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed_topic_list: None (default value)\n",
    "# A list of seed words per topic to converge around\n",
    "\n",
    "\n",
    "#all-mpnet-base-v2\n",
    "topics, probs = topic_model.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab440170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'topic': topics, 'document': data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-catalyst",
   "metadata": {},
   "source": [
    "### Topic Model evaluation: coherence & diversity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-replacement",
   "metadata": {},
   "source": [
    "### Saving the model inside the COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ne fonctionne pas sur WS\n",
    "#topic_model.save(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-framework",
   "metadata": {},
   "source": [
    "We are using a buffer as a temporary file, to put the data inside the function pu_object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp buffer\n",
    "file_buffer = io.BytesIO()\n",
    "csv_buffer = StringIO()\n",
    "\n",
    "\n",
    "#converting topic_model and stocking inside file_buffer\n",
    "#The embedding model is save with the model\n",
    "joblib.dump(topic_model, file_buffer)\n",
    "\n",
    "#Same for the df to csv\n",
    "df.to_csv(csv_buffer,header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-brisbane",
   "metadata": {},
   "source": [
    "Saving the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the file_buffer content inside the COS\n",
    "cos.Bucket(BUCKET).put_object(Key=\"model_without_vectorizer_10000data.joblib\", Body= file_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-mediterranean",
   "metadata": {},
   "source": [
    "Saving the Dataframe as csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the df content inside the COS\n",
    "cos.Bucket(BUCKET).put_object(Key='df_model.csv', Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-mozambique",
   "metadata": {},
   "source": [
    "### Loading the model from the COS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-retirement",
   "metadata": {},
   "source": [
    "Here, we are loading the model with a TemporyFile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryFile() as temp_file:\n",
    "    #download the model into temp file\n",
    "    cos.Object(BUCKET, \"model_without_vectorizer_10000data.joblib\").download_fileobj(temp_file)\n",
    "    temp_file.seek(0)\n",
    "    #load into joblib\n",
    "    model=joblib.load(temp_file)\n",
    "topic_model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b332375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-technician",
   "metadata": {},
   "source": [
    "Then, the Dataframe as a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = io.StringIO(get_item_cos(BUCKET,'df_model.csv').decode(\"utf-8\"))\n",
    "df=pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-flashing",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info()\n",
    "freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_topic = freq.shape[0]-1\n",
    "print(f\"there is {nb_topic} topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['topic']==1].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(1) # Select the most frequent topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_document=topic_model.get_representative_docs()\n",
    "representative_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e397e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.generate_topic_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-lithuania",
   "metadata": {},
   "source": [
    "### Evaluation topic modeling, coherence & diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_word(topics, topic_model):\n",
    "    list_word_topic= list()\n",
    "    list_topic = set(topics)\n",
    "    #Iteration to get the top n word for each topic\n",
    "    for topic in list_topic:\n",
    "        tmp=list() #list of word for a topic + the associated label\n",
    "        tmp.append(str(topic))\n",
    "        #tmp2 is the list of word with cTF-IFD score, but we just want the word\n",
    "        tmp2 = topic_model.get_topic(topic)\n",
    "        for el in tmp2 :\n",
    "            if len(el[0])>3 : #filtering to only keep the word up to 3\n",
    "                tmp.append(el[0])\n",
    "        #list_word_topic is a list[list]\n",
    "        list_word_topic.append(tmp)\n",
    "    return list_word_topic             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all-mpnet-base-v2\n",
    "list_word_topic = get_topic_word(topics, topic_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e12d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-millennium",
   "metadata": {},
   "source": [
    "The coherence measure is relative to the dataset. For each model fitted to this dataset, we will get a coherence measure :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a5111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for topic coherence calculation\n",
    "#To get the token + string check + lower (cases sensitive)\n",
    "tokenizer = lambda s: re.findall( '\\w+', str((s.lower())) )\n",
    "data_tokenised = [ tokenizer(t) for t in data ]\n",
    "\n",
    "from gensim.models import Phrases\n",
    "bigram = Phrases(data, min_count=10)\n",
    "\n",
    "for idx in range(len(data_tokenised)):\n",
    "    for token in bigram[data_tokenised[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            data_tokenised[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ace8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(data_tokenised)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.2)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in data_tokenised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2131f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13815bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary with the vocabulary\n",
    "word2id = Dictionary(data_tokenised)\n",
    "\n",
    "def get_coherence(list_word_topic):\n",
    "    # Coherence model\n",
    "    cm = CoherenceModel(topics=list_word_topic, \n",
    "                        texts=data_tokenised,\n",
    "                        coherence='c_v',  \n",
    "                        dictionary=word2id)\n",
    "\n",
    "    #1st : coherence for each topic\n",
    "    coherence_per_topic = cm.get_coherence_per_topic()\n",
    "    \n",
    "    #2nd :global coherence \n",
    "    coherence = cm.get_coherence()\n",
    "    print(\"The coherence per topic is \",coherence_per_topic )\n",
    "    print(\"The topic model coherence is \",coherence )\n",
    "    return coherence, coherence_per_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f2c666",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#all-mpnet-base-v2\n",
    "coherence, coherence_per_topic = get_coherence(list_word_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the result\n",
    "topics_str = [ '\\n '.join(t[:4]) for t in list_word_topic ] #we are printing just the number and the 3 first word\n",
    "data_topic_score = pd.DataFrame( data=zip(topics_str, coherence_per_topic), columns=['Topic', 'Coherence'] )\n",
    "data_topic_score = data_topic_score.set_index('Topic')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(nb_topic/3,nb_topic) )\n",
    "ax.set_title(\"Topics coherence\\n $C_v$\")\n",
    "sns.heatmap(data=data_topic_score, annot=True, square=True,\n",
    "            cmap='Reds', fmt='.2f',\n",
    "            linecolor='black', ax=ax )\n",
    "plt.yticks( rotation=0 )\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-adapter",
   "metadata": {},
   "source": [
    "### Topic Reduction after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-greek",
   "metadata": {},
   "source": [
    "(BERT documentation : https://maartengr.github.io/BERTopic/getting_started/topicreduction/topicreduction.html#visualize-probablities)\n",
    "\n",
    "As there is to much predected intents, we will reduced them:\n",
    " - First, let see them in 2 dimenssion, topic intersection's is a good way to find the potentiel merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-disposition",
   "metadata": {},
   "source": [
    "Then let see how the BERTopic function can reduce them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further reduce topics\n",
    "#update data2!! put the data again\n",
    "new_topics, new_probs = topic_model.reduce_topics(data, topics, nr_topics=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-eight",
   "metadata": {},
   "source": [
    "As we can see above, the merging is not really optimal, there is still a lot of intersection who could be merge. A Data Analyst coul do that job by hand? Can we do something more to merge? May be by using the Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the cosine similarity matrix between topic embeddings,\n",
    "#a heatmap is created showing the similarity between topics.\n",
    "topic_model.visualize_heatmap()\n",
    "\n",
    "#to save it:\n",
    "#fig = topic_model.visualize_heatmap()\n",
    "#fig.write_html(\"path/to/file.html\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1341b4e1",
   "metadata": {},
   "source": [
    "Working on the output. We want a list of accurate document who are discribing our topics. The goal is 15 exemple per topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df53125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representative_docs(df) : \n",
    "    list_topic = set(topics)\n",
    "    for topic in list_topic : \n",
    "        topic_doc = df[df.topic == topic]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89968f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
